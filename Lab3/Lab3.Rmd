---
title: "Lab3"
author: "Damian Ke & Kyriakos Papadopoulos"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Question 1: Stable distribution

## 1.1
```{r, warning=FALSE}
library(poweRlaw)
library(ggplot2)
library(gridExtra)


target_function <- function(x, c){
  res = c * sqrt(2 * pi) ^ (-1) * exp((-c^2) / (2*x)) * x ^ (-3/2)
}

majority_function <- function(x, a, t_min){
  res <- c()
  
  for(i in 1:length(x)){
    if(x[i] <= t_min){
      res[i] = ((a - 1) / t_min) * (t_min/t_min) ^ (-a)
    }
    else{
      res[i] <- ((a - 1) / t_min) * (x[i]/t_min) ^ (-a)
    }
  }
  
  return(res)
}

t_min=1.5
c=1.5
alpha=1.5


```


**Question 1: ** Plot f(x) and fp(x) together. Can the power-law distribution be used just by itself or is
there a problem at any place of the support Explain what the problem is and how can it
be taken care of. Provide values of the powerilaw distribution's parameters that can be
used in the acceptance-rejection algorithm. Derive and implement a majorizing density.

**Answer 1: ** The problem is the missing values at fp(x) from 0 to t_min. In that interval the power law distribution cant generate values(it generates values which are equal to infinitive). Solution for this issue is to find another distribution or line that envelopts the target function.


After testing visually many times with many different values of different parameters we ended up with these parameters for our power-law distribution for x > t_min:

- **t_min=1.5**
- **c=1.5**
- **alpha=1.5**

This makes the supported majorizing density envelop the target density. Also the look quite similar to each other with these parameters and for x > t_min have almost the same values.

For the interval of (0, t_min) we use the power-law distribution with input the value of t_min and for this interval we will have a constant y of Tmin as we can see in the graph.

We will find 2 majorizing constants. One for x <= t_min and one for x > t_min. The process for finding both will be the same. We have to find the minimum c, as it's suggested in the **slide 18** that satisfies this condition: $$c >= \frac{f_x(x)}{f_y(x)}$$

We will implement the above condition.

```{r}
ggplot() +
  xlim(0.1, 10) +
  geom_function(fun = target_function, args = list(c = c), colour = "red") +
  geom_function(fun = majority_function, colour = "black", args = list(a=alpha,t_min=t_min))
```

## 1.2

**Question 2: ** Implement an acceptance-rejection algorithm for sampling from the one-sided strictly
stable distribution of order 1=2 with the proposal distribution built around the power-
law distribution. To sample from a power-law distribution you can use the function
poweRlaw::rplcon().

**Answer 2: **

```{r}

x <- seq(0.001, 100, 0.001)
x1 <- x[x<=t_min]

prob <- sum(majority_function(x1,alpha ,t_min)) / sum((majority_function(x, alpha, t_min)))
print(prob)

# For x bigger than t_min
maj_c1 <- max(target_function(x[x>t_min],c) / majority_function(x[x>t_min], alpha, t_min))

# For x smaller than t_min

maj_c2 <- max(target_function(x[x<=t_min],c) / majority_function(x[x<=t_min], alpha, t_min))

n = 5000

accept_reject <- function(n,c){
  values <- c()
  rejects <- 0
  for(i in 1:n){
    x <- NA
    
    while(is.na(x)){
      prob2 <- sample(0:1, 1, prob = c(prob, 1-prob))
      # For 0 is x <= tmin
      # For 1 is x > tmin
      u <- runif(1)
      
      if(prob2 == 0){
        y <- runif(1, 0, t_min)
        
        if(u <= target_function(y, c) / (maj_c2 * majority_function(y, alpha, t_min))){
          x <- y
          values <- append(values, x)
        }else{
          rejects <- rejects + 1
        }
      }
      else{
        y <- rplcon(1, xmin = t_min, alpha = alpha)
        
        if(u <= target_function(y, c) / (maj_c1 * majority_function(y, alpha, t_min))){
          x <- y
          values <- append(values, x)
        }else{
          rejects <- rejects + 1
        }
      }
      
    }
    
  }
  return(c(values,rejects))
}

```



## 1.3


**Question 3: ** Generate a large sample using your implemented sampler, for different choices of c. Present
the sample graphically. What is the mean and variance and how do they depend on c?
Study the rejection rate.

**Answer 3:** 

```{r}
c_list = c(1,2,3,4,5,6)
results = data.frame(matrix(NA,ncol=length(c_list),nrow=n))
summarized_results = data.frame(matrix(NA,ncol=length(c_list),nrow=3))
plots = c()
df = data.frame()
for (i in 1:length(c_list)) {
  var <- c_list[i]
  answer = accept_reject(n,var)
  rejects = answer[n+1]
  results[i] = answer[1:n]
  summarized_results[1,i] = format(mean(answer[answer <= 200]), digits=10)
  summarized_results[2,i] = format(var(answer[answer <= 200]), digits=10)
  summarized_results[3,i] = format(rejects/n, digits=10)
  df = data.frame(results)
}
colnames(results) = c("c1","c2","c3","c4","c5","c6")
colnames(df) = c("c1","c2","c3","c4","c5","c6")
colnames(summarized_results) = c("c1","c2","c3","c4","c5","c6")
rownames(summarized_results) = c("Mean","Variance","Rejection Rate")


create_plot <- function(i){
  title = paste("c = ", i, sep = "")
  
  hist(results[i][results[i] < 200], col="blue", breaks=70, xlab="", 
       ylab="sample density", freq=FALSE, main=title, xlim=c(1,30), ylim=c(0,0.10))
}

# Code to show all the graphs
par(mfrow=c(3,2))
plots = sapply(1:6, create_plot)

knitr::kable(summarized_results, align = c("l", "l", "l", "l", "l", "l"))
```

In order to be able to understand how mean, variance and rejection rate depend on c, we remove outlying numbers, more specifically all the numbers that are bigger than 200.

We can see that mean, variance and rejection rate depend on c, as all the 3 of them increase, as c increases. If we hadn't removed the oytlying number we wouldn't be able to spot the difference 


# Question 2: Laplace distribution

**Question 1: **Write a code generating double exponential distribution DR(0, 1) 
from Unif(0, 1) by using the inverse CDF method. Explain how you obtained that 
code step by step. Generate 10000 random numbers from this distribution, plot the 
histogram and comment whether the result looks reasonable.

**Answer 1: **

- For $x < \mu$

$$CDF = \frac{a}{2} \left[\int_{-\infty}^{x} e^{\alpha x - a\mu} \; dx\right] =>$$


$$CDF = \frac{e^{\alpha x - a\mu}}{2}(1)$$

- For $x >= \mu$
$$CDF = \frac{a}{2} \left[\int_{-\infty}^{x} e^{- \alpha x + a\mu} \; dx\right] =>$$
$$CDF = 1+ \frac{e^{-\alpha x +a\mu}}{2} (2)$$

**Taking into account the (1) and (2) math equations of CDF we can write it as:**
$$\frac{1}{2} + \frac{1}{2}sgn(x-\mu)(1 - e^{-a|x-\mu|})$$

The next step is to find the inverse CDF which we are gonna represent it like 
$F^{-1}(y)$. In order to find we define the CDF as equal to y.

$$y = \frac{1}{2} + \frac{1}{2}sgn(x-\mu)(1 - e^{-a|x-\mu|})$$

Now, we have to solve the above questions with respect to x 

- For $x>= \mu$

$$x = \mu - \frac{1}{a}ln(2-2y) (3)$$

- For $x < \mu$

$$x = \mu + \frac{1}{a}ln(2y) (4)$$

**Taking into account the (3) and (4) math equations of  inverse CDF we can write it as:**

$$F^{-1}(p) = \mu - \frac{1}{a} sgn(p-0.5)ln(1-2|p-0.5|)$$


```{r}
de <- function(x){
  y <- (1/2) * exp(-abs(x))
  return(y)
}

x <- runif(10000)

invcdf <- function(x){
  out <- 0 - 1 * sign(x - 0.5) * log(1-2*abs(x - 0.5), base = exp(1))
}

set.seed(12345)
x <- runif(10000, min=0, max= 1)
y1 = de(x)
y2 = invcdf(x)

df = data.frame(y1, y2, x)
df[is.na(df)] = 0

p <- ggplot(df, aes(x=y2)) + geom_histogram(color= "black", binwidth = 0.1)
p
```

Visually we can say that the samples that we got from the inverse CDF follow the distribution of the Laplace distribution. So the result looks reasonable as it's very similar to Laplace distribution.

**Question 2: **  Use the Acceptance/rejection method with DE(0, 1) as a majorizing 
density to generate N (0, 1) variables. Explain step by step how this was done. 
How did you choose constant c in this method? Generate 2000 random numbers N(0, 1) 
using your code and plot the histogram. Compute the average rejection rate R in 
the acceptance/rejection procedure. What is the expected rejection rate ER and 
how close is it to R? Generate 2000 numbers from N (0, 1) using standard rnorm() 
procedure, plot the histogram and compare the obtained two histograms.

**Answer 2: **

The first step is to find the majorizing constant that will lead to

$$cf_Y(x) >= f_X(x)$$

Replacing the function we end up with this equation: 

$$c >= \frac{2}{sqrt(2\pi)} e^{\frac{-x^2}{2} + |x|}$$

Taking the first and second derivative we find that it maximazes at x= 1. So we replace x=1 and c it's equal to $$\frac{2}{sqrt(2*\pi)} * e^{0.5}$$

```{r}
c = (2/sqrt(2*pi)) * exp(0.5)
n = 2000

results <- c()
rejects <- 0

for(i in 1:n){
  
  continue <- TRUE
  
  while(continue){
      y <- invcdf(runif(1))
      u <- runif(1)
      if( u <= (dnorm(y,mean =  0, sd = 1) / (c * de(y)))){
        results <- append(results, y)
        continue = FALSE
      }
      else{
        rejects <- rejects + 1
      }
  }

}

normald = rnorm(2000, 0, 1)

df = data.frame(results)
p <- ggplot(df, aes(x=results)) + geom_histogram(color="black", binwidth = 0.1,
                                                 fill = "blue") + ggtitle("Acceptance Rejectance Method") +
                                                 xlab("") + ylab("")

norm = rnorm(2000,0, 1)
df2 = data.frame(norm)
p1 <- ggplot(df2, aes(x=norm)) + geom_histogram(color="black", binwidth = 0.1,
                                                  fill = "red") + ggtitle("Normal Distribution") +
                                                  xlab("") + ylab("")

rejection_rate <- rejects / n
expected_rejection_rate <- 1 - (1/c)

grid.arrange(p, p1, ncol=2)
```

We can see that the samples that we obtained follow the distribution

```{r, echo=FALSE}
cat("Our rejection rate is ", rejection_rate,"\n")
cat("Our expected rejection rate is ", expected_rejection_rate, "\n")
cat("Their difference is ", abs(rejection_rate - expected_rejection_rate))
```

The steps that we followed were to find c, which we explain above how we did that. 
After that we iterated 2000 times as defined in the exercise. In each iteration 
we take a random sample, y, from the Laplace distribution through the inverse CDF(as x
we use a sample from an uniform distribution) and another sample, u, from an uniform
distribution. After that we compare if u  is smaller than the normal distribution
given y, devided by the the product of the majority constant and of the inverse 
CDF of laplace distribution given y. If it's true we accept y as a sample otherwise
we repeat the process again with new u and y until we get an acceptable y.
